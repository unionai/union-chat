cache_workflow:
  hf_secret_key: huggingface-api-key  # update this with the secret key for your huggingface api key

global_config:
  project: <project_name>  # change the project name here
  domain: development  # update this to the desired domain

streamlit:
  resources:
    cpu: "2"
    mem: 3Gi

models:
  - display_name: Qwen2.5 0.5B
    model_id: qwen2.5:0.5b
    base_url: http://localhost:11434
    local: true

  - display_name: Qwen2.5-0.5B-Instruct
    name: qwen25-0-5b
    model_id: Qwen/Qwen2.5-0.5B-Instruct
    llm_runtime:
      image: ghcr.io/unionai-oss/serving-vllm:0.1.17
      llm_type: vllm
      resources:
        cpu: 7
        mem: 25Gi
        gpu: 1
      accelerator: nvidia-l4
      stream_model: true
      env:
        VLLM_DISABLE_COMPILE_CACHE: "1"
    cache_version: v1
    max_tokens: null

  - display_name: Phi-3.5-mini-Instruct
    name: phi-3-5-mini-instruct
    model_id: microsoft/Phi-3.5-mini-instruct
    llm_runtime:
      image: ghcr.io/unionai-oss/serving-sglang:0.1.17
      llm_type: sglang
      resources:
        cpu: 7
        mem: 25Gi
        gpu: 1
      accelerator: nvidia-l4
      stream_model: true
      extra_args: --attention-backend triton
    cache_version: v1
    max_tokens: null

  - display_name: Llama3.2 3B
    name: llama-3-2-3b
    model_id: meta-llama/Llama-3.2-3B
    llm_runtime:
      image: ghcr.io/unionai-oss/serving-sglang:0.1.17
      llm_type: sglang
      resources:
        cpu: 7
        mem: 25Gi
        gpu: 1
      accelerator: nvidia-l4
      stream_model: true
      extra_args: --attention-backend triton
    cache_version: v1
    max_tokens: null
